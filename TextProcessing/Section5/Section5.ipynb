{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "32326b4f",
            "metadata": {},
            "source": [
                "N-Gram\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "e4719ca5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.util import ngrams\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from collections import Counter\n",
                "from nltk.collocations import *\n",
                "\n",
                "#------- The Sample ot text-------\n",
                "text=['I am happy because I am learning',\n",
                "      \"I want to eat English food\",\n",
                "      \"I want to eat Chinese food\",\n",
                "     ]\n",
                "\n",
                "#------- The Normal Way-------\n",
                "def Create_gram(text,n):\n",
                "    t=\" \".join(text)\n",
                "    tokens = t.split()\n",
                "    #print([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
                "    print([tokens[i:i+n] for i in range(len(tokens)-n+1)])\n",
                "    print([\" \".join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "946f2799",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[['I', 'am'], ['am', 'happy'], ['happy', 'because'], ['because', 'I'], ['I', 'am'], ['am', 'learning'], ['learning', 'I'], ['I', 'want'], ['want', 'to'], ['to', 'eat'], ['eat', 'English'], ['English', 'food'], ['food', 'I'], ['I', 'want'], ['want', 'to'], ['to', 'eat'], ['eat', 'Chinese'], ['Chinese', 'food']]\n",
                        "['I am', 'am happy', 'happy because', 'because I', 'I am', 'am learning', 'learning I', 'I want', 'want to', 'to eat', 'eat English', 'English food', 'food I', 'I want', 'want to', 'to eat', 'eat Chinese', 'Chinese food']\n"
                    ]
                }
            ],
            "source": [
                "Create_gram(text,2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e41be93",
            "metadata": {},
            "outputs": [],
            "source": [
                "#======== The First Way============\n",
                "def gram(my_text):\n",
                "    my_text = \" \".join(my_text)\n",
                "    words = nltk.word_tokenize(my_text)\n",
                "    my_bigrams = nltk.bigrams(words)\n",
                "    my_trigrams = nltk.trigrams(words)\n",
                "    print([' '.join(grams) for grams in my_bigrams])\n",
                "    # print([' '.join(grams) for grams in my_trigrams])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a84781e",
            "metadata": {},
            "outputs": [],
            "source": [
                "gram(text)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "b75fc26d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1-gram:  ['I', 'am', 'happy', 'because', 'I', 'am', 'learning', 'I', 'want', 'to', 'eat', 'English', 'food', 'I', 'want', 'to', 'eat', 'Chinese', 'food']\n",
                        "2-gram:  ['I am', 'am happy', 'happy because', 'because I', 'I am', 'am learning', 'learning I', 'I want', 'want to', 'to eat', 'eat English', 'English food', 'food I', 'I want', 'want to', 'to eat', 'eat Chinese', 'Chinese food']\n",
                        "3-gram:  ['I am happy', 'am happy because', 'happy because I', 'because I am', 'I am learning', 'am learning I', 'learning I want', 'I want to', 'want to eat', 'to eat English', 'eat English food', 'English food I', 'food I want', 'I want to', 'want to eat', 'to eat Chinese', 'eat Chinese food']\n",
                        "7-gram:  ['I am happy because I am learning', 'am happy because I am learning I', 'happy because I am learning I want', 'because I am learning I want to', 'I am learning I want to eat', 'am learning I want to eat English', 'learning I want to eat English food', 'I want to eat English food I', 'want to eat English food I want', 'to eat English food I want to', 'eat English food I want to eat', 'English food I want to eat Chinese', 'food I want to eat Chinese food']\n"
                    ]
                }
            ],
            "source": [
                "#======== The Second Way============\n",
                "def extract_gram(data,num):\n",
                "    data = \" \".join(data)\n",
                "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
                "    return [\" \".join(grams) for grams in n_grams]\n",
                "\n",
                "\n",
                "print(\"1-gram: \", extract_gram(text, 1))\n",
                "print(\"2-gram: \", extract_gram(text, 2))\n",
                "print(\"3-gram: \", extract_gram(text, 3))\n",
                "print(\"7-gram: \", extract_gram(text, 7))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "f345da94",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['am happy', 'happy because', 'because am', 'am learning', 'learning want', 'want to', 'to eat', 'eat english', 'english food', 'food want', 'want to', 'to eat', 'eat chinese', 'chinese food', 'am happy because', 'happy because am', 'because am learning', 'am learning want', 'learning want to', 'want to eat', 'to eat english', 'eat english food', 'english food want', 'food want to', 'want to eat', 'to eat chinese', 'eat chinese food', 'am happy because am', 'happy because am learning', 'because am learning want', 'am learning want to', 'learning want to eat', 'want to eat english', 'to eat english food', 'eat english food want', 'english food want to', 'food want to eat', 'want to eat chinese', 'to eat chinese food']\n"
                    ]
                }
            ],
            "source": [
                "#========== The Third Way============\n",
                "def n_gram(text):\n",
                "    t = \" \".join(text)\n",
                "    vec = CountVectorizer(ngram_range=(2,4))\n",
                "    analyzer = vec.build_analyzer()\n",
                "    print (analyzer(t))\n",
                "\n",
                "n_gram(text)\n",
                "    \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "2adc7548",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "('I', 'am', 'happy') 1\n",
                        "('am', 'happy', 'because') 1\n",
                        "('happy', 'because', 'I') 1\n",
                        "('because', 'I', 'am') 1\n",
                        "('I', 'am', 'learning') 1\n",
                        "('am', 'learning', 'I') 1\n",
                        "('learning', 'I', 'want') 1\n",
                        "('I', 'want', 'to') 2\n",
                        "('want', 'to', 'eat') 2\n",
                        "('to', 'eat', 'English') 1\n",
                        "('eat', 'English', 'food') 1\n",
                        "('English', 'food', 'I') 1\n",
                        "('food', 'I', 'want') 1\n",
                        "('to', 'eat', 'Chinese') 1\n",
                        "('eat', 'Chinese', 'food') 1\n"
                    ]
                }
            ],
            "source": [
                "#============= The Fourth Way===========\n",
                "def grams(text):\n",
                "    text=\" \".join(text)\n",
                "    trigrams=nltk.collocations.TrigramCollocationFinder.from_words(text.split())\n",
                "    for trigram, freq in trigrams.ngram_fd.items():\n",
                "        print (trigram, freq)\n",
                "    \n",
                "grams(text)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aeb48dbb",
            "metadata": {},
            "source": [
                "Counting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd1c6b19",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys(['I am', 'am happy', 'happy because', 'because I', 'am learning', 'learning I', 'I want', 'want to', 'to eat', 'eat English', 'English food', 'food I', 'eat Chinese', 'Chinese food'])\n",
                        "dict_values([2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1])\n",
                        "I am 2\n",
                        "am happy 1\n",
                        "happy because 1\n",
                        "because I 1\n",
                        "am learning 1\n",
                        "learning I 1\n",
                        "I want 2\n",
                        "want to 2\n",
                        "to eat 2\n",
                        "eat English 1\n",
                        "English food 1\n",
                        "food I 1\n",
                        "eat Chinese 1\n",
                        "Chinese food 1\n"
                    ]
                }
            ],
            "source": [
                "#=========== Counting============\n",
                "def Count(pairs):\n",
                "    fd=nltk.FreqDist(pairs)\n",
                "    print(fd.keys())\n",
                "    print(fd.values())\n",
                "    for k,v in fd.items():\n",
                "        print (k,v)\n",
                "\n",
                "Count(extract_gram(text,2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "adb15b08",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Counter({'I am': 2, 'I want': 2, 'want to': 2, 'to eat': 2, 'am happy': 1, 'happy because': 1, 'because I': 1, 'am learning': 1, 'learning I': 1, 'eat English': 1, 'English food': 1, 'food I': 1, 'eat Chinese': 1, 'Chinese food': 1})\n",
                        "0\n"
                    ]
                }
            ],
            "source": [
                "def Count2():\n",
                "    bigram= extract_gram(text,2)\n",
                "    print (Counter(bigram))\n",
                "    print(Counter(bigram)[\"I\"])\n",
                "\n",
                "Count2()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c1a3378d",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
